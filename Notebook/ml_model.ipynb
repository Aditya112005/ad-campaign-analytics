{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15af4ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3af461a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.94      0.90      1953\n",
      "           1       0.25      0.17      0.20       217\n",
      "           2       0.14      0.11      0.12       110\n",
      "           3       0.19      0.17      0.18        71\n",
      "           4       0.03      0.02      0.02        50\n",
      "           5       0.03      0.02      0.02        47\n",
      "           6       0.12      0.09      0.10        46\n",
      "           7       0.00      0.00      0.00        30\n",
      "           8       0.05      0.04      0.04        25\n",
      "           9       0.00      0.00      0.00        15\n",
      "          10       0.00      0.00      0.00        22\n",
      "          11       0.00      0.00      0.00        20\n",
      "          12       0.00      0.00      0.00         7\n",
      "          13       0.00      0.00      0.00        16\n",
      "          14       0.00      0.00      0.00         8\n",
      "          15       0.00      0.00      0.00        11\n",
      "          16       0.12      0.10      0.11        10\n",
      "          17       0.07      0.08      0.08        12\n",
      "          18       0.00      0.00      0.00         8\n",
      "          19       0.00      0.00      0.00         9\n",
      "          20       0.17      0.11      0.13         9\n",
      "          21       0.00      0.00      0.00         4\n",
      "          22       0.00      0.00      0.00         4\n",
      "          23       0.00      0.00      0.00         3\n",
      "          24       0.00      0.00      0.00         8\n",
      "          25       0.00      0.00      0.00        10\n",
      "          26       0.00      0.00      0.00         2\n",
      "          27       0.00      0.00      0.00         2\n",
      "          28       0.00      0.00      0.00         5\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00         2\n",
      "          31       0.00      0.00      0.00         5\n",
      "          32       0.00      0.00      0.00         7\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.50      0.33      0.40         3\n",
      "          36       0.00      0.00      0.00         2\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         6\n",
      "          40       0.00      0.00      0.00         2\n",
      "          41       0.00      0.00      0.00         3\n",
      "          42       0.00      0.00      0.00         5\n",
      "          43       0.00      0.00      0.00         4\n",
      "          44       0.00      0.00      0.00         3\n",
      "          45       0.00      0.00      0.00         6\n",
      "          46       0.00      0.00      0.00         6\n",
      "          47       0.00      0.00      0.00         5\n",
      "          48       0.00      0.00      0.00         1\n",
      "          49       0.00      0.00      0.00         0\n",
      "          50       0.00      0.00      0.00         4\n",
      "          51       0.00      0.00      0.00         3\n",
      "          52       0.00      0.00      0.00         2\n",
      "          53       0.00      0.00      0.00         3\n",
      "          55       0.00      0.00      0.00         1\n",
      "          56       0.00      0.00      0.00         4\n",
      "          57       0.00      0.00      0.00         2\n",
      "          58       0.00      0.00      0.00         4\n",
      "          59       0.00      0.00      0.00         4\n",
      "          60       0.00      0.00      0.00         3\n",
      "          61       0.00      0.00      0.00         2\n",
      "          62       0.00      0.00      0.00         1\n",
      "          63       0.00      0.00      0.00         1\n",
      "          64       0.00      0.00      0.00         3\n",
      "          65       0.00      0.00      0.00         1\n",
      "          66       0.00      0.00      0.00         3\n",
      "          67       0.00      0.00      0.00         0\n",
      "          68       0.00      0.00      0.00         0\n",
      "          69       0.00      0.00      0.00         2\n",
      "          70       0.00      0.00      0.00         3\n",
      "          71       0.00      0.00      0.00         2\n",
      "          72       0.00      0.00      0.00         2\n",
      "          73       0.00      0.00      0.00         1\n",
      "          74       0.00      0.00      0.00         2\n",
      "          75       0.00      0.00      0.00         3\n",
      "          76       0.00      0.00      0.00         2\n",
      "          77       0.00      0.00      0.00         1\n",
      "          78       0.00      0.00      0.00         1\n",
      "          79       0.00      0.00      0.00         1\n",
      "          80       0.00      0.00      0.00         3\n",
      "          81       0.00      0.00      0.00         0\n",
      "          82       0.00      0.00      0.00         3\n",
      "          83       0.00      0.00      0.00         0\n",
      "          84       0.00      0.00      0.00         1\n",
      "          85       0.00      0.00      0.00         0\n",
      "          86       0.00      0.00      0.00         0\n",
      "          87       0.00      0.00      0.00         0\n",
      "          88       0.00      0.00      0.00         1\n",
      "          89       0.00      0.00      0.00         2\n",
      "          90       0.00      0.00      0.00         0\n",
      "          91       0.00      0.00      0.00         3\n",
      "          92       0.00      0.00      0.00         2\n",
      "          93       0.00      0.00      0.00         1\n",
      "          94       0.00      0.00      0.00         0\n",
      "          95       0.00      0.00      0.00         4\n",
      "          96       0.00      0.00      0.00         1\n",
      "          97       0.00      0.00      0.00         0\n",
      "          98       0.00      0.00      0.00         0\n",
      "         101       0.00      0.00      0.00         1\n",
      "         102       0.00      0.00      0.00         0\n",
      "         103       0.00      0.00      0.00         0\n",
      "         104       0.00      0.00      0.00         1\n",
      "         105       0.00      0.00      0.00         0\n",
      "         107       0.00      0.00      0.00         0\n",
      "         108       0.00      0.00      0.00         1\n",
      "         111       0.00      0.00      0.00         2\n",
      "         114       0.00      0.00      0.00         1\n",
      "         117       0.00      0.00      0.00         0\n",
      "         119       0.00      0.00      0.00         1\n",
      "         120       0.00      0.00      0.00         0\n",
      "         122       0.00      0.00      0.00         1\n",
      "         123       0.00      0.00      0.00         1\n",
      "         125       0.00      0.00      0.00         1\n",
      "         127       0.00      0.00      0.00         2\n",
      "         131       0.00      0.00      0.00         0\n",
      "         132       0.00      0.00      0.00         1\n",
      "         133       0.00      0.00      0.00         0\n",
      "         135       0.00      0.00      0.00         0\n",
      "         136       0.00      0.00      0.00         0\n",
      "         137       0.00      0.00      0.00         0\n",
      "         139       0.00      0.00      0.00         1\n",
      "         140       0.00      0.00      0.00         1\n",
      "         141       0.00      0.00      0.00         0\n",
      "         143       0.00      0.00      0.00         1\n",
      "         145       0.00      0.00      0.00         0\n",
      "         146       0.00      0.00      0.00         1\n",
      "         148       0.00      0.00      0.00         1\n",
      "         156       0.00      0.00      0.00         1\n",
      "         157       0.00      0.00      0.00         1\n",
      "         158       0.00      0.00      0.00         1\n",
      "         161       0.00      0.00      0.00         1\n",
      "         162       0.00      0.00      0.00         1\n",
      "         165       0.00      0.00      0.00         1\n",
      "         166       0.00      0.00      0.00         0\n",
      "         167       0.00      0.00      0.00         1\n",
      "         169       0.00      0.00      0.00         1\n",
      "         172       0.00      0.00      0.00         1\n",
      "         173       0.00      0.00      0.00         1\n",
      "         174       0.00      0.00      0.00         1\n",
      "         175       0.00      0.00      0.00         1\n",
      "         179       0.00      0.00      0.00         1\n",
      "         181       0.00      0.00      0.00         2\n",
      "         185       0.00      0.00      0.00         0\n",
      "         189       0.00      0.00      0.00         3\n",
      "         192       0.00      0.00      0.00         0\n",
      "         193       0.00      0.00      0.00         1\n",
      "         194       0.00      0.00      0.00         0\n",
      "         197       0.00      0.00      0.00         0\n",
      "         200       0.00      0.00      0.00         1\n",
      "         201       0.00      0.00      0.00         1\n",
      "         205       0.00      0.00      0.00         0\n",
      "         206       0.00      0.00      0.00         1\n",
      "         207       0.00      0.00      0.00         1\n",
      "         209       0.00      0.00      0.00         1\n",
      "         210       0.00      0.00      0.00         1\n",
      "         211       0.00      0.00      0.00         1\n",
      "         212       0.00      0.00      0.00         1\n",
      "         215       0.00      0.00      0.00         1\n",
      "         218       0.00      0.00      0.00         1\n",
      "         219       0.00      0.00      0.00         0\n",
      "         220       0.00      0.00      0.00         0\n",
      "         221       0.00      0.00      0.00         0\n",
      "         222       0.00      0.00      0.00         0\n",
      "         223       0.00      0.00      0.00         0\n",
      "         224       0.00      0.00      0.00         1\n",
      "         226       0.00      0.00      0.00         1\n",
      "         228       0.00      0.00      0.00         1\n",
      "         229       0.00      0.00      0.00         1\n",
      "         231       0.00      0.00      0.00         0\n",
      "         233       0.00      0.00      0.00         1\n",
      "         234       0.00      0.00      0.00         0\n",
      "         235       0.00      0.00      0.00         0\n",
      "         236       0.00      0.00      0.00         1\n",
      "         237       0.00      0.00      0.00         1\n",
      "         239       0.00      0.00      0.00         1\n",
      "         241       0.00      0.00      0.00         0\n",
      "         244       0.00      0.00      0.00         1\n",
      "         247       0.00      0.00      0.00         1\n",
      "         249       0.00      0.00      0.00         0\n",
      "         250       0.00      0.00      0.00         1\n",
      "         251       0.00      0.00      0.00         1\n",
      "         254       0.00      0.00      0.00         0\n",
      "         256       0.00      0.00      0.00         1\n",
      "         257       0.00      0.00      0.00         1\n",
      "         258       0.00      0.00      0.00         1\n",
      "         259       0.00      0.00      0.00         1\n",
      "         260       0.00      0.00      0.00         1\n",
      "         261       0.00      0.00      0.00         1\n",
      "         262       0.00      0.00      0.00         2\n",
      "         263       0.00      0.00      0.00         0\n",
      "         264       0.00      0.00      0.00         0\n",
      "         266       0.00      0.00      0.00         1\n",
      "         267       0.00      0.00      0.00         0\n",
      "         268       0.00      0.00      0.00         1\n",
      "         269       0.00      0.00      0.00         0\n",
      "         270       0.00      0.00      0.00         1\n",
      "         271       0.00      0.00      0.00         0\n",
      "         274       0.00      0.00      0.00         0\n",
      "         275       0.00      0.00      0.00         0\n",
      "         278       0.00      0.00      0.00         0\n",
      "         279       0.00      0.00      0.00         1\n",
      "         281       0.00      0.00      0.00         0\n",
      "         282       0.00      0.00      0.00         1\n",
      "         283       0.00      0.00      0.00         1\n",
      "         287       0.00      0.00      0.00         0\n",
      "         289       0.00      0.00      0.00         1\n",
      "         291       0.00      0.00      0.00         1\n",
      "         295       0.00      0.00      0.00         2\n",
      "         296       0.00      0.00      0.00         0\n",
      "         297       0.00      0.00      0.00         0\n",
      "         298       0.00      0.00      0.00         0\n",
      "         301       0.00      0.00      0.00         0\n",
      "         302       0.00      0.00      0.00         0\n",
      "         303       0.00      0.00      0.00         0\n",
      "         304       0.00      0.00      0.00         1\n",
      "         305       0.00      0.00      0.00         1\n",
      "         308       0.00      0.00      0.00         1\n",
      "         310       0.00      0.00      0.00         0\n",
      "         312       0.00      0.00      0.00         1\n",
      "         314       0.00      0.00      0.00         1\n",
      "         317       0.00      0.00      0.00         1\n",
      "         321       0.00      0.00      0.00         1\n",
      "         322       0.00      0.00      0.00         2\n",
      "         327       0.00      0.00      0.00         0\n",
      "         328       0.00      0.00      0.00         1\n",
      "         332       0.00      0.00      0.00         0\n",
      "         334       0.00      0.00      0.00         2\n",
      "         337       0.00      0.00      0.00         1\n",
      "         338       0.00      0.00      0.00         1\n",
      "         340       0.00      0.00      0.00         2\n",
      "         341       0.00      0.00      0.00         0\n",
      "         342       0.00      0.00      0.00         1\n",
      "         344       0.00      0.00      0.00         0\n",
      "         346       0.00      0.00      0.00         0\n",
      "         355       0.00      0.00      0.00         0\n",
      "         356       0.00      0.00      0.00         0\n",
      "         357       0.00      0.00      0.00         1\n",
      "         358       0.00      0.00      0.00         0\n",
      "         361       0.00      0.00      0.00         1\n",
      "         362       0.00      0.00      0.00         0\n",
      "         365       0.00      0.00      0.00         0\n",
      "         366       0.00      0.00      0.00         0\n",
      "         369       0.00      0.00      0.00         0\n",
      "         373       0.00      0.00      0.00         1\n",
      "         374       0.00      0.00      0.00         0\n",
      "         375       0.00      0.00      0.00         1\n",
      "         381       0.00      0.00      0.00         1\n",
      "         382       0.00      0.00      0.00         1\n",
      "         383       0.00      0.00      0.00         1\n",
      "         390       1.00      1.00      1.00         1\n",
      "         392       0.00      0.00      0.00         1\n",
      "         396       0.00      0.00      0.00         1\n",
      "         397       0.00      0.00      0.00         0\n",
      "         399       0.00      0.00      0.00         1\n",
      "         401       0.00      0.00      0.00         0\n",
      "         403       0.00      0.00      0.00         1\n",
      "         406       0.00      0.00      0.00         0\n",
      "         408       0.00      0.00      0.00         1\n",
      "         409       0.00      0.00      0.00         0\n",
      "         410       0.00      0.00      0.00         0\n",
      "         416       0.00      0.00      0.00         1\n",
      "         417       0.00      0.00      0.00         0\n",
      "         419       0.00      0.00      0.00         1\n",
      "         420       0.00      0.00      0.00         1\n",
      "         423       0.00      0.00      0.00         0\n",
      "         425       0.00      0.00      0.00         1\n",
      "         426       0.00      0.00      0.00         1\n",
      "         428       0.00      0.00      0.00         1\n",
      "         429       0.00      0.00      0.00         1\n",
      "         431       0.00      0.00      0.00         1\n",
      "         433       0.00      0.00      0.00         0\n",
      "         434       0.00      0.00      0.00         0\n",
      "         435       0.00      0.00      0.00         1\n",
      "         438       0.00      0.00      0.00         0\n",
      "         439       0.00      0.00      0.00         0\n",
      "         441       0.00      0.00      0.00         0\n",
      "         443       0.00      0.00      0.00         2\n",
      "         449       0.00      0.00      0.00         0\n",
      "         451       0.00      0.00      0.00         1\n",
      "         454       0.00      0.00      0.00         1\n",
      "         455       0.00      0.00      0.00         0\n",
      "         456       0.00      0.00      0.00         1\n",
      "         457       0.00      0.00      0.00         0\n",
      "         460       0.00      0.00      0.00         1\n",
      "         463       0.00      0.00      0.00         1\n",
      "         464       0.00      0.00      0.00         1\n",
      "         465       0.00      0.00      0.00         1\n",
      "         469       0.00      0.00      0.00         0\n",
      "         470       0.00      0.00      0.00         0\n",
      "         472       0.00      0.00      0.00         1\n",
      "         473       0.00      0.00      0.00         0\n",
      "         475       0.00      0.00      0.00         1\n",
      "         476       0.00      0.00      0.00         0\n",
      "         477       0.00      0.00      0.00         0\n",
      "         479       0.00      0.00      0.00         1\n",
      "         480       0.00      0.00      0.00         1\n",
      "         482       0.00      0.00      0.00         0\n",
      "         483       0.00      0.00      0.00         1\n",
      "         486       0.00      0.00      0.00         0\n",
      "         487       0.00      0.00      0.00         2\n",
      "         488       0.00      0.00      0.00         0\n",
      "         489       0.00      0.00      0.00         1\n",
      "         494       0.00      0.00      0.00         1\n",
      "         497       0.00      0.00      0.00         0\n",
      "         500       0.00      0.00      0.00         1\n",
      "         501       0.00      0.00      0.00         0\n",
      "         507       0.00      0.00      0.00         2\n",
      "         508       0.00      0.00      0.00         0\n",
      "         509       0.00      0.00      0.00         1\n",
      "         511       0.00      0.00      0.00         1\n",
      "         512       0.00      0.00      0.00         1\n",
      "         519       0.00      0.00      0.00         1\n",
      "         520       0.00      0.00      0.00         1\n",
      "         521       0.00      0.00      0.00         0\n",
      "         522       0.00      0.00      0.00         1\n",
      "         523       0.00      0.00      0.00         0\n",
      "         526       0.00      0.00      0.00         1\n",
      "         528       0.00      0.00      0.00         1\n",
      "         529       0.00      0.00      0.00         0\n",
      "         530       0.00      0.00      0.00         0\n",
      "         531       0.00      0.00      0.00         0\n",
      "         533       0.00      0.00      0.00         1\n",
      "         534       0.00      0.00      0.00         1\n",
      "         535       0.00      0.00      0.00         1\n",
      "         537       0.00      0.00      0.00         1\n",
      "         539       0.00      0.00      0.00         0\n",
      "         540       0.00      0.00      0.00         0\n",
      "         542       0.00      0.00      0.00         0\n",
      "         543       0.00      0.00      0.00         1\n",
      "         544       0.00      0.00      0.00         1\n",
      "         548       0.00      0.00      0.00         0\n",
      "         551       0.00      0.00      0.00         0\n",
      "         555       0.00      0.00      0.00         0\n",
      "         559       0.00      0.00      0.00         1\n",
      "         561       0.00      0.00      0.00         1\n",
      "         565       0.00      0.00      0.00         0\n",
      "         566       0.00      0.00      0.00         0\n",
      "         567       0.00      0.00      0.00         1\n",
      "         570       0.00      0.00      0.00         0\n",
      "         571       0.00      0.00      0.00         1\n",
      "         574       0.00      0.00      0.00         0\n",
      "         575       0.00      0.00      0.00         1\n",
      "         579       0.00      0.00      0.00         1\n",
      "         580       0.00      0.00      0.00         1\n",
      "         584       0.00      0.00      0.00         1\n",
      "         590       0.00      0.00      0.00         1\n",
      "         591       0.00      0.00      0.00         1\n",
      "         594       0.00      0.00      0.00         1\n",
      "         595       0.00      0.00      0.00         1\n",
      "         597       0.00      0.00      0.00         1\n",
      "         599       0.00      0.00      0.00         0\n",
      "         600       0.00      0.00      0.00         1\n",
      "         601       0.00      0.00      0.00         1\n",
      "         602       0.00      0.00      0.00         0\n",
      "         604       0.00      0.00      0.00         1\n",
      "         605       0.00      0.00      0.00         0\n",
      "         607       0.00      0.00      0.00         0\n",
      "         612       0.00      0.00      0.00         1\n",
      "         613       0.00      0.00      0.00         0\n",
      "         617       0.00      0.00      0.00         1\n",
      "         625       0.00      0.00      0.00         0\n",
      "         628       0.00      0.00      0.00         1\n",
      "         629       0.00      0.00      0.00         0\n",
      "         632       0.00      0.00      0.00         1\n",
      "         633       0.00      0.00      0.00         0\n",
      "         634       0.00      0.00      0.00         1\n",
      "         635       0.00      0.00      0.00         1\n",
      "         641       0.00      0.00      0.00         0\n",
      "         643       0.00      0.00      0.00         0\n",
      "         657       0.00      0.00      0.00         1\n",
      "         658       0.00      0.00      0.00         0\n",
      "         659       0.00      0.00      0.00         1\n",
      "         660       0.00      0.00      0.00         1\n",
      "         663       0.00      0.00      0.00         1\n",
      "         664       0.00      0.00      0.00         0\n",
      "         672       0.00      0.00      0.00         1\n",
      "         676       0.00      0.00      0.00         0\n",
      "         678       0.00      0.00      0.00         0\n",
      "         682       0.00      0.00      0.00         1\n",
      "         683       0.00      0.00      0.00         0\n",
      "         687       0.00      0.00      0.00         1\n",
      "         693       0.00      0.00      0.00         0\n",
      "         694       0.00      0.00      0.00         0\n",
      "         696       0.00      0.00      0.00         1\n",
      "         698       0.00      0.00      0.00         1\n",
      "         700       0.00      0.00      0.00         0\n",
      "         709       0.00      0.00      0.00         1\n",
      "         710       0.00      0.00      0.00         1\n",
      "         714       0.00      0.00      0.00         1\n",
      "         716       0.00      0.00      0.00         0\n",
      "         721       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.62      3082\n",
      "   macro avg       0.01      0.01      0.01      3082\n",
      "weighted avg       0.58      0.62      0.60      3082\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1839   64   18 ...    0    0    0]\n",
      " [ 141   36   15 ...    0    0    0]\n",
      " [  61   15   12 ...    0    0    0]\n",
      " ...\n",
      " [   0    0    0 ...    0    0    0]\n",
      " [   0    0    0 ...    0    0    0]\n",
      " [   0    0    0 ...    0    0    0]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes in y_true not equal to the number of columns in 'y_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m y_prob \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# ROC-AUC Score for multi-class\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m roc_auc \u001b[38;5;241m=\u001b[39m roc_auc_score(y_test, y_prob, multi_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124movr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROC-AUC Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, roc_auc)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Save model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:634\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m multi_class \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_class must be in (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124movo\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124movr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 634\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _multiclass_roc_auc_score(\n\u001b[0;32m    635\u001b[0m         y_true, y_score, labels, multi_class, average, sample_weight\n\u001b[0;32m    636\u001b[0m     )\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    638\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_true)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:751\u001b[0m, in \u001b[0;36m_multiclass_roc_auc_score\u001b[1;34m(y_true, y_score, labels, multi_class, average, sample_weight)\u001b[0m\n\u001b[0;32m    749\u001b[0m     classes \u001b[38;5;241m=\u001b[39m _unique(y_true)\n\u001b[0;32m    750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(classes) \u001b[38;5;241m!=\u001b[39m y_score\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 751\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    752\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of classes in y_true not equal to the number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    753\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_score\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    754\u001b[0m         )\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_class \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124movo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Number of classes in y_true not equal to the number of columns in 'y_score'"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\LENOVO\\\\OneDrive\\\\Desktop\\\\ad-campaign-analytics\\\\data\\\\online_advertising_performance_data.csv\")\n",
    "\n",
    "# Target column\n",
    "target = 'post_click_conversions'\n",
    "\n",
    "# Drop missing target values\n",
    "df = df.dropna(subset=[target])\n",
    "\n",
    "# Encode categorical variables\n",
    "le_campaign = LabelEncoder()\n",
    "le_placement = LabelEncoder()\n",
    "le_banner = LabelEncoder()\n",
    "\n",
    "df['campaign_enc'] = le_campaign.fit_transform(df['campaign_number'].astype(str))\n",
    "df['placement_enc'] = le_placement.fit_transform(df['placement'].astype(str))\n",
    "df['banner_enc'] = le_banner.fit_transform(df['banner'].astype(str))\n",
    "\n",
    "# Encode user_engagement\n",
    "le_engage = LabelEncoder()\n",
    "df['engagement_enc'] = le_engage.fit_transform(df['user_engagement'].astype(str))\n",
    "\n",
    "# Define features and target\n",
    "X = df[['campaign_enc', 'placement_enc', 'banner_enc', 'revenue', 'cost', 'clicks', 'engagement_enc']]\n",
    "\n",
    "# Encode target variable (in case it's not already numeric or cleanly encoded)\n",
    "le_target = LabelEncoder()\n",
    "y = le_target.fit_transform(df[target])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train RandomForest model\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_prob = clf.predict_proba(X_test)\n",
    "\n",
    "# ROC-AUC Score for multi-class\n",
    "roc_auc = roc_auc_score(y_test, y_prob, multi_class='ovr')\n",
    "print(\"ROC-AUC Score:\", roc_auc)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(clf, \"post_click_rf_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3da2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, f1_score\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_and_validate_data(file_path):\n",
    "    \"\"\"Load and validate the dataset\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
    "        print(f\"Columns: {df.columns.tolist()}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found. Please check the file path.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocess and clean the data\"\"\"\n",
    "    # Define expected columns with fallbacks\n",
    "    column_mapping = {\n",
    "        'target': ['post_click_conversions', 'post_click', 'conversion', 'converted'],\n",
    "        'campaign': ['campaign_number', 'campaign', 'campaign_id'],\n",
    "        'placement': ['placement', 'placement_id'],\n",
    "        'banner': ['banner', 'banner_size', 'banner_id'],\n",
    "        'revenue': ['revenue', 'total_revenue'],\n",
    "        'cost': ['cost', 'total_cost'],\n",
    "        'clicks': ['clicks', 'total_clicks'],\n",
    "        'engagement': ['user_engagement', 'engagement', 'user_enga']\n",
    "    }\n",
    "    \n",
    "    # Find actual column names\n",
    "    actual_columns = {}\n",
    "    for key, possible_names in column_mapping.items():\n",
    "        for name in possible_names:\n",
    "            if name in df.columns:\n",
    "                actual_columns[key] = name\n",
    "                break\n",
    "        if key not in actual_columns:\n",
    "            print(f\"Warning: No column found for {key}. Available columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Check if target column exists\n",
    "    if 'target' not in actual_columns:\n",
    "        print(\"Error: Target column not found!\")\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    target_col = actual_columns['target']\n",
    "    \n",
    "    # Handle missing values in target\n",
    "    initial_rows = len(df)\n",
    "    df = df.dropna(subset=[target_col])\n",
    "    print(f\"Dropped {initial_rows - len(df)} rows with missing target values\")\n",
    "    \n",
    "    # Convert target to binary if needed\n",
    "    if df[target_col].dtype == 'object':\n",
    "        df[target_col] = df[target_col].astype(str).str.lower()\n",
    "        df[target_col] = df[target_col].map({'yes': 1, 'true': 1, '1': 1, 'no': 0, 'false': 0, '0': 0})\n",
    "    \n",
    "    # Ensure target is binary\n",
    "    unique_values = df[target_col].unique()\n",
    "    if len(unique_values) > 2:\n",
    "        print(f\"Warning: Target has more than 2 unique values: {unique_values}\")\n",
    "        # Convert to binary (1 if > 0, else 0)\n",
    "        df[target_col] = (df[target_col] > 0).astype(int)\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    encoders = {}\n",
    "    feature_columns = []\n",
    "    \n",
    "    for key in ['campaign', 'placement', 'banner']:\n",
    "        if key in actual_columns:\n",
    "            col = actual_columns[key]\n",
    "            # Handle missing values in categorical columns\n",
    "            df[col] = df[col].fillna('unknown')\n",
    "            \n",
    "            # Create encoder\n",
    "            encoder = LabelEncoder()\n",
    "            encoded_col = f\"{key}_enc\"\n",
    "            df[encoded_col] = encoder.fit_transform(df[col].astype(str))\n",
    "            encoders[key] = encoder\n",
    "            feature_columns.append(encoded_col)\n",
    "            print(f\"Encoded {col} -> {encoded_col} ({len(encoder.classes_)} unique values)\")\n",
    "    \n",
    "    # Handle numeric features\n",
    "    numeric_features = []\n",
    "    for key in ['revenue', 'cost', 'clicks', 'engagement']:\n",
    "        if key in actual_columns:\n",
    "            col = actual_columns[key]\n",
    "            # Convert to numeric and handle missing values\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "            numeric_features.append(col)\n",
    "            feature_columns.append(col)\n",
    "    \n",
    "    # Create derived features\n",
    "    if 'cost' in actual_columns and 'clicks' in actual_columns:\n",
    "        cost_col = actual_columns['cost']\n",
    "        clicks_col = actual_columns['clicks']\n",
    "        # Cost per click (handle division by zero)\n",
    "        df['cpc'] = df[cost_col] / df[clicks_col].replace(0, np.nan)\n",
    "        df['cpc'] = df['cpc'].fillna(df['cpc'].median())\n",
    "        feature_columns.append('cpc')\n",
    "        print(\"Added derived feature: cpc (cost per click)\")\n",
    "    \n",
    "    if 'revenue' in actual_columns and 'cost' in actual_columns:\n",
    "        revenue_col = actual_columns['revenue']\n",
    "        cost_col = actual_columns['cost']\n",
    "        # Return on investment\n",
    "        df['roi'] = (df[revenue_col] - df[cost_col]) / df[cost_col].replace(0, np.nan)\n",
    "        df['roi'] = df['roi'].fillna(df['roi'].median())\n",
    "        feature_columns.append('roi')\n",
    "        print(\"Added derived feature: roi (return on investment)\")\n",
    "    \n",
    "    # Final feature selection\n",
    "    available_features = [col for col in feature_columns if col in df.columns]\n",
    "    X = df[available_features]\n",
    "    y = df[target_col].astype(int)\n",
    "    \n",
    "    print(f\"Final feature set: {available_features}\")\n",
    "    print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
    "    \n",
    "    return X, y, encoders, available_features, df\n",
    "\n",
    "def train_and_evaluate_model(X, y):\n",
    "    \"\"\"Train and evaluate the Random Forest model\"\"\"\n",
    "    # Check class distribution\n",
    "    class_counts = y.value_counts()\n",
    "    print(f\"Class distribution: {class_counts.to_dict()}\")\n",
    "    \n",
    "    if len(class_counts) < 2:\n",
    "        print(\"Error: Only one class present in target variable!\")\n",
    "        return None\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set size: {X_train.shape[0]}\")\n",
    "    print(f\"Test set size: {X_test.shape[0]}\")\n",
    "    \n",
    "    # Train model with class balancing\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        class_weight='balanced',  # Handle class imbalance\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2\n",
    "    )\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_pred_proba = clf.predict_proba(X_test)\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MODEL EVALUATION RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "    \n",
    "    if y_pred_proba.shape[1] > 1:  # Binary classification\n",
    "        print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba[:, 1]):.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': clf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Feature Importances:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    return clf, feature_importance\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Load data\n",
    "    df = load_and_validate_data(\"C:\\\\Users\\\\LENOVO\\\\OneDrive\\\\Desktop\\\\ad-campaign-analytics\\\\data\\\\online_advertising_performance_data.csv\")\n",
    "    if df is None:\n",
    "        print(\"Please ensure your dataset file exists and is accessible.\")\n",
    "        return\n",
    "    \n",
    "    # Preprocess data\n",
    "    X, y, encoders, feature_names, processed_df = preprocess_data(df)\n",
    "    if X is None:\n",
    "        print(\"Data preprocessing failed. Please check your dataset structure.\")\n",
    "        return\n",
    "    \n",
    "    # Train and evaluate model\n",
    "    model = train_and_evaluate_model(X, y)\n",
    "    if model is None:\n",
    "        print(\"Model training failed.\")\n",
    "        return\n",
    "    \n",
    "    clf, feature_importance = model\n",
    "    \n",
    "    # Save model and encoders\n",
    "    try:\n",
    "        joblib.dump({\n",
    "            'model': clf,\n",
    "            'encoders': encoders,\n",
    "            'feature_names': feature_names\n",
    "        }, \"post_click_rf_model.joblib\")\n",
    "        print(\"\\nModel saved successfully as 'post_click_rf_model.joblib'\")\n",
    "        \n",
    "        # Save feature importance\n",
    "        feature_importance.to_csv(\"feature_importance.csv\", index=False)\n",
    "        print(\"Feature importance saved as 'feature_importance.csv'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
